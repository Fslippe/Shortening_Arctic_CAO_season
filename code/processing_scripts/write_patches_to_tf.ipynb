{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cores: 256\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 15:57:16.059885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14678 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:25:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import calendar\n",
    "from write_to_tf import write_tfrecord\n",
    "# Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\"  , len(logical_gpus), \"Logical GPUs\")\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "bands=[31]\n",
    "#max_vals = np.load(\"%smodels/max_vals_dnb_l95_z50_ps128_(29)_cao_months_202012-202111.npy\" %data_loc)\n",
    "print(len(bands))\n",
    "\n",
    "#encoder = load_model(\"/uio/hume/student-u37/fslippe/data/models/winter_2020_21_band(6,20,29)_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2019\n",
      "Year: 2020\n",
      "Year: 2021\n",
      "Year: 2022\n",
      "Year: 2023\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "bands = [31]\n",
    "patch_size = 128\n",
    "band_str = \"31\"\n",
    "model_run_name = f\"dnb_ice01_l95_z50_ps128_band31\" # ex.\n",
    "yearly_data = True\n",
    "\n",
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "months = [1, 2, 3, 4, 10, 11, 12]\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "data_loc = \"/dir_to_your_data/\"\n",
    "model_folder_save = \"/dir_to_save_min_and_max/\"\n",
    "model_tfdata_save = \"/dir_to_save_tf_data_used_for_model_training/\"\n",
    "os.makedirs(model_folder_save, exist_ok=True)\n",
    "os.makedirs(model_tfdata_save, exist_ok=True)\n",
    "\n",
    "# Load numpy arrays containing training and test patches\n",
    "for year in years:\n",
    "    print(\"Year:\", year)\n",
    "    if yearly_data:\n",
    "        train_data.append(np.load(f\"{data_loc}/train_{model_run_name}_{year}.npy\"))\n",
    "        test_data.append(np.load(f\"{data_loc}/test_{model_run_name}_{year}.npy\"))\n",
    "    else:\n",
    "        for month in months:\n",
    "            _, days_in_month = calendar.monthrange(year, month)\n",
    "            for day in range(1, days_in_month+1):\n",
    "                date = str(year) + str(month).zfill(2) + str(day).zfill(2)\n",
    "                try:\n",
    "                    train_data.append(np.load(f\"{data_loc}/train_{model_run_name}_{date}.npz\")[\"train_patches\"])\n",
    "                    test_data.append(np.load(f\"{data_loc}/test_{model_run_name}_{date}.npz\")[\"test_patches\"])\n",
    "                    # if not os.path.exists(f\"/scratch/fslippe/modis/training_data/band31/test_{model_run_name}_{date}.npz\"):\n",
    "                    #     print(f\"File     not found for date {date}\")\n",
    "\n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"File not found for date {date}: {e}\")\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of patches: 32582\n",
      "max: 11.319851\n",
      "min: 1.0262215\n"
     ]
    }
   ],
   "source": [
    "# Combine all\n",
    "train_data = np.concatenate((train_data), axis=0)\n",
    "test_data = np.concatenate((test_data), axis=0)\n",
    "max_val = np.max(train_data)\n",
    "min_val = np.min(train_data)\n",
    "\n",
    "print(\"number of patches:\", len(train_data))\n",
    "print(\"max:\", max_val)\n",
    "print(\"min:\", min_val)\n",
    "\n",
    "np.save(model_folder_save + \"max_val_%s\" %(model_run_name) + f\"_{np.min(years)}-{np.max(years)}\", max_val)\n",
    "np.save(model_folder_save + \"min_val_%s\" %(model_run_name) + f\"_{np.min(years)}-{np.max(years)}\", min_val)\n",
    "\n",
    "normlized_train_data = (train_data - min_val) / (max_val - min_val)\n",
    "normlized_test_data = (test_data - min_val) / (max_val - min_val)\n",
    "np.save(model_tfdata_save + \"normalized_valpatches_%s\" %(model_run_name), normlized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000\n",
      "/scratch/fslippe/modis/training_data/tf_data/dnb_l95_z50_ps384_band31/_0.tfrecord\n",
      "1 10000\n",
      "/scratch/fslippe/modis/training_data/tf_data/dnb_l95_z50_ps384_band31/_1.tfrecord\n",
      "2 12582\n",
      "/scratch/fslippe/modis/training_data/tf_data/dnb_l95_z50_ps384_band31/_2.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# patches_per_file = int(200000 * (128/patch_size)**2)\n",
    "patches_per_file = 100000\n",
    "\n",
    "tot_files = int(len(normlized_train_data) / patches_per_file)\n",
    "\n",
    "for i in range(tot_files):\n",
    "    if i < tot_files-1:\n",
    "        chunk = normlized_train_data[i*patches_per_file: i*patches_per_file+patches_per_file]\n",
    "    else:\n",
    "        chunk = normlized_train_data[i*patches_per_file: ]\n",
    "    print(i, len(chunk))\n",
    "    print(f\"{model_tfdata_save}_{i}.tfrecord\")\n",
    "    write_tfrecord(model_tfdata_save + f\"normalized_trainingpatches_{model_run_name}_{i}.tfrecord\", chunk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
